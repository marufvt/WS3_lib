{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5772b21b-07ad-42ef-8c95-9b5176d2dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "SALIENCY_DIR = '/data/amartyadutta/sensitivity_exp/'\n",
    "MODEL_FLAG = 'original' # 'noisy'\n",
    "INPUT_FMT = 'crop_accumulate' # 'crop', 'mask', 'crop_mask'\n",
    "NUM_CROPS = 200\n",
    "size= 224\n",
    "NUM_MASKS = 50\n",
    "# if INPUT_FMT == 'crop_mask':\n",
    "#     NUM_MASKS = 20\n",
    "# else:\n",
    "#     NUM_MASKS = 50\n",
    "\n",
    "# SALIENCY_DIR = '/raid/maruf/WSS/SALIENCY/original/image_crop/'\n",
    "# SALIENCY_DIR = '/raid/maruf/WSS/SALIENCY/original/image_binmask/'\n",
    "# SALIENCY_DIR = '/raid/maruf/WSS/SALIENCY/original/image_binmask_crop/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcb0169-317e-4b48-9438-07788f00c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.autograd import Variable\n",
    "from torch import multiprocessing, cuda\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.backends import cudnn\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import voc12.dataloader\n",
    "from misc import torchutils, imutils, myutils\n",
    "import torchvision.transforms as transforms\n",
    "from misc.myutils import unnormalize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e28eab-90a0-425e-8942-09aaa36db154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num GPUs 6\n"
     ]
    }
   ],
   "source": [
    "GPU = True\n",
    "if GPU == True:\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5'\n",
    "    print(\"num GPUs\",torch.cuda.device_count())\n",
    "else:\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594557e5-1fb8-476f-8240-f8b6fe1fd56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSNAME_LIST = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "        'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "        'cow', 'diningtable', 'dog', 'horse',\n",
    "        'motorbike', 'person', 'pottedplant',\n",
    "        'sheep', 'sofa', 'train',\n",
    "        'tvmonitor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b508a7e-508d-4fd9-9c60-88f40df09046",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6daeb60b-a4bb-4ade-8b12-3ab9744e8fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainercv.datasets import VOCSemanticSegmentationDataset\n",
    "from chainercv.evaluations import calc_semantic_segmentation_confusion\n",
    "\n",
    "# ground truth labels in gt_labels\n",
    "\n",
    "gt_dataset = VOCSemanticSegmentationDataset(split=\"train\",\n",
    "                                            data_dir='/home/amartyadutta/VOC12/AMN/Datasets/VOCdevkit/VOC2012')\n",
    "\n",
    "gt_labels = [gt_dataset.get_example_by_keys(i, (1,))[0] for i in range(len(gt_dataset))]\n",
    "\n",
    "\n",
    "# segmentation dataset and data_loader \n",
    "\n",
    "dataset = voc12.dataloader.VOC12ClassificationDatasetMSF(img_name_list_path='/home/amartyadutta/VOC12/AMN/voc12/train.txt',\n",
    "                                                         voc12_root='/home/amartyadutta/VOC12/AMN/Datasets/VOCdevkit/VOC2012', \n",
    "                                                         scales=(1.0,))\n",
    "\n",
    "data_loader = DataLoader(dataset, shuffle=False)\n",
    "\n",
    "\n",
    "# NDR gt corresponds to non-deterministic region ground truths\n",
    "\n",
    "ndr_gt_labels = [np.load(os.path.join('/raid/maruf/WSS/NDR_ground_truth', id + '.npy'), allow_pickle=True) for id in gt_dataset.ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6330c305-7cb4-44e7-b0ca-8402f304dc5a",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0522142-e0a1-4dde-adb0-7f30040e469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(importlib.import_module('net.resnet50_cam'), 'CAM_original')()\n",
    "\n",
    "if MODEL_FLAG == 'original':\n",
    "    model.load_state_dict(torch.load('/home/maruf/ws2m2/sess/res50_cam_original_version.pth' + '.pth', map_location='cpu'), strict=True)\n",
    "\n",
    "elif MODEL_FLAG == 'noisy':\n",
    "    model.load_state_dict(torch.load('/home/maruf/ws2m2/sess/res50_cam_bin_noise_version.pth' + '.pth', map_location='cpu'), strict=True)\n",
    "    \n",
    "model.eval()\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14a9c5-680b-447a-b64f-36d2c810775b",
   "metadata": {},
   "source": [
    "# SALIENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b814dcc9-e3f0-45e4-8b01-c8f3dba42282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RandomCropDataset(Dataset):\n",
    "    def __init__(self, tensor, transform, size=1):\n",
    "        self.tensor = tensor.squeeze(0)\n",
    "        self.transform = transform\n",
    "        self.size = size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.tensor)\n",
    "\n",
    "    \n",
    "def get_saliencies(\n",
    "    img_tensor,\n",
    "    target_class,\n",
    "    transform,\n",
    "    input_fmt='img',\n",
    "    num_crops=200,\n",
    "    num_masks = 50\n",
    "):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if input_fmt == 'img':\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        img_tensor.requires_grad = True\n",
    "\n",
    "        scores = model(img_tensor.type(dtype))\n",
    "\n",
    "        combined_saliencies = []\n",
    "\n",
    "        for class_idx in target_class:\n",
    "\n",
    "            target_score = scores[:, class_idx]\n",
    "\n",
    "            saliency = torch.autograd.grad(\n",
    "                target_score, img_tensor,\n",
    "                grad_outputs = torch.ones_like(target_score),\n",
    "                retain_graph = True\n",
    "            )[0]\n",
    "\n",
    "            saliency = saliency.detach().abs().max(1)[0]\n",
    "            combined_saliencies.append(saliency)\n",
    "\n",
    "        raw_saliency = torch.cat(combined_saliencies).detach()\n",
    "        \n",
    "        return raw_saliency\n",
    "    \n",
    "    elif input_fmt == 'crop_accumulate':\n",
    "        \n",
    "        img_tensor.requires_grad = True\n",
    "        \n",
    "        dataset = RandomCropDataset(img_tensor, transform, size=num_crops)\n",
    "        dataloader = DataLoader(dataset, batch_size=100)\n",
    "\n",
    "        _, _, H, W = img_tensor.shape\n",
    "        num_classes = len(target_class)\n",
    "        real_saliencies = torch.zeros(num_classes, 1, H, W).type(dtype)\n",
    "        for batch in dataloader:\n",
    "            model.zero_grad()\n",
    "            out = model(batch.type(dtype))\n",
    "    \n",
    "            for i, label in enumerate(target_class):\n",
    "                label = label.long()\n",
    "                target_val = out[:,label]\n",
    "                weights = torch.sigmoid(target_val)\n",
    "\n",
    "                # Weighted Saliency\n",
    "                saliency = torch.autograd.grad(\n",
    "                                        target_val, img_tensor, \n",
    "                                        grad_outputs=weights,\n",
    "                                        retain_graph=True\n",
    "                                    )[0]\n",
    "                saliency = saliency.detach().abs().max(1)[0]\n",
    "                real_saliencies[i] += saliency.type(dtype)\n",
    "\n",
    "        raw_saliency = real_saliencies.squeeze(1).detach()\n",
    "        \n",
    "        return raw_saliency\n",
    "        \n",
    "        \n",
    "    elif input_fmt == 'mask':\n",
    "        \n",
    "        _, _, H, W = img_tensor.shape\n",
    "        num_classes = len(target_class)\n",
    "        agg_saliencies = torch.zeros(num_classes, H, W).type(dtype)\n",
    "        \n",
    "        for _ in range(num_masks):\n",
    "            \n",
    "            prob_mask = 0.9 * torch.ones(H, W)\n",
    "            binary_mask = torch.bernoulli(prob_mask).detach()\n",
    "            binary_mask.requires_grad = True\n",
    "            \n",
    "            noisy_img = img_tensor * binary_mask\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            scores = model(noisy_img.type(dtype))\n",
    "\n",
    "            combined_saliencies = []\n",
    "\n",
    "            for class_idx in target_class:\n",
    "\n",
    "                target_score = scores[:, class_idx]\n",
    "\n",
    "                saliency = torch.autograd.grad(\n",
    "                    target_score, binary_mask,\n",
    "                    grad_outputs = torch.ones_like(target_score),\n",
    "                    retain_graph = True\n",
    "                )[0]\n",
    "\n",
    "                saliency = saliency.detach().abs()\n",
    "                \n",
    "                combined_saliencies.append(saliency.unsqueeze(0))\n",
    "                \n",
    "            raw_saliency = torch.cat(combined_saliencies)\n",
    "            \n",
    "            agg_saliencies += raw_saliency.type(dtype)\n",
    "            \n",
    "        agg_saliencies /= num_masks\n",
    "        \n",
    "        return agg_saliencies\n",
    "    \n",
    "    elif input_fmt == 'crop_mask':\n",
    "        \n",
    "        _, _, H, W = img_tensor.shape\n",
    "        num_classes = len(target_class)\n",
    "        agg_saliencies = torch.zeros(num_classes, H, W).type(dtype)\n",
    "        \n",
    "        for _ in range(num_masks):\n",
    "            \n",
    "            prob_mask = 0.9 * torch.ones(H, W)\n",
    "            binary_mask = torch.bernoulli(prob_mask).detach()\n",
    "            binary_mask.requires_grad = True\n",
    "            \n",
    "            noisy_img = img_tensor * binary_mask\n",
    "            \n",
    "            #####\n",
    "            dataset = RandomCropDataset(noisy_img, transform, size=num_crops)\n",
    "            dataloader = DataLoader(dataset, batch_size=100)\n",
    "\n",
    "            # _, _, H, W = img_tensor.shape\n",
    "            # num_classes = len(target_class)\n",
    "            real_saliencies = torch.zeros(num_classes, H, W).type(dtype)\n",
    "\n",
    "            for batch in dataloader:\n",
    "                model.zero_grad()\n",
    "                out = model(batch.type(dtype))\n",
    "                for i, label in enumerate(target_class):\n",
    "                    label = label.long()\n",
    "                    target_val = out[:,label]\n",
    "                    weights = torch.sigmoid(target_val)\n",
    "                    saliency = torch.autograd.grad(\n",
    "                                            target_val, binary_mask, \n",
    "                                            grad_outputs=weights,\n",
    "                                            retain_graph=True\n",
    "                                        )[0]\n",
    "                    saliency = saliency.detach().abs()\n",
    "                    \n",
    "                    real_saliencies[i] += saliency.type(dtype)\n",
    "\n",
    "            raw_saliency = real_saliencies.detach()\n",
    "            #####\n",
    "            \n",
    "            agg_saliencies += raw_saliency.type(dtype)\n",
    "                    \n",
    "        agg_saliencies /= num_masks\n",
    "        \n",
    "        return agg_saliencies\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59465481-92cf-4bbb-a7f1-6f4970eaa733",
   "metadata": {},
   "source": [
    "# Save Saliencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e382d0d-fa12-462b-9c51-524f46c5d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iter, pack in enumerate(tqdm(data_loader)):\n",
    "        \n",
    "#     img_name = pack['name'][0]\n",
    "    \n",
    "#     if os.path.exists(os.path.join(SALIENCY_DIR, img_name+'.npy')):\n",
    "#         continue\n",
    "\n",
    "#     label = pack['label'][0]\n",
    "    \n",
    "#     target_class = torch.nonzero(label)[:, 0]\n",
    "    \n",
    "#     input_img = pack['img'][0][0:1, ...]\n",
    "    \n",
    "#     with torch.autocast(device_type = 'cuda', dtype = torch.float16):\n",
    "#         saliency = get_saliencies(img_tensor=input_img,\n",
    "#                                   target_class=target_class,\n",
    "#                                   input_fmt=INPUT_FMT,\n",
    "#                                   num_masks=NUM_MASKS)\n",
    "\n",
    "#         np.save(os.path.join(SALIENCY_DIR, img_name+'.npy'), \n",
    "#                     {'saliency': saliency.cpu().numpy(), 'cls_label': target_class})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4005845c-a670-429a-86e8-ad3b71945364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a344ae8-aa89-40b1-9c6b-28479be6fdab",
   "metadata": {},
   "source": [
    "# Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f654f304-5b84-4b56-9a5e-0195537b2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(input_saliency):\n",
    "    '''\n",
    "    tensor -> tensor\n",
    "    input_saliency.shape = Num_class, H, W\n",
    "    output_saliency.shape = Num_class, H, W\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    input_saliency = input_saliency.unsqueeze(1)\n",
    "    \n",
    "    kernel_size, sigma = 13, 5\n",
    "    \n",
    "    axis = torch.linspace(-(kernel_size-1)/2., \n",
    "                          -(kernel_size-1)/2., \n",
    "                          kernel_size)\n",
    "    gaussian = torch.exp(- 0.5 * axis**2 / sigma**2)\n",
    "    \n",
    "    kernel = torch.outer(gaussian, gaussian)[None, None, :, :]\n",
    "    \n",
    "    output_saliency = F.conv2d(input_saliency, kernel, padding=kernel_size//2)\n",
    "    \n",
    "    return output_saliency.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80653eb0-cb33-45f3-a8a1-408c39d70b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(input_saliency):\n",
    "    '''\n",
    "    tensor -> tensor\n",
    "    input_saliency.shape = Num_class, H, W\n",
    "    output_saliency.shape = Num_class, H, W\n",
    "    \n",
    "    '''\n",
    "    N_CLASS, H, W = input_saliency.shape\n",
    "    \n",
    "    input_saliency = input_saliency.unsqueeze(1)\n",
    "    \n",
    "    min_ = input_saliency.view(N_CLASS, -1).min(-1)[0]\n",
    "    min_ = min_[:, None, None, None].repeat(1, 1, H, W)\n",
    "    \n",
    "    max_ = torch.quantile(input = input_saliency.view(N_CLASS,-1), \n",
    "                          q = 0.995, \n",
    "                          dim=-1, keepdim=False, \n",
    "                          interpolation=\"nearest\", out=None)\n",
    "    max_ = max_[:, None, None, None].repeat(1, 1, H, W)\n",
    "    \n",
    "    saliency = torch.clamp(input_saliency, max=max_)\n",
    "    saliency_normalized = (saliency - min_)/(max_ - min_)\n",
    "    \n",
    "    return saliency_normalized.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4620eb6f-ee38-47c2-9cb6-10a7b6a13102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtraction(input_saliency, weights=[1.5, 0.5]):\n",
    "    '''\n",
    "    tensor -> tensor\n",
    "    input_saliency.shape = Num_class, H, W\n",
    "    output_saliency.shape = Num_class, H, W\n",
    "    \n",
    "    '''\n",
    "    N_CLASS, H, W = input_saliency.shape\n",
    "    \n",
    "    input_saliency = input_saliency.unsqueeze(1)\n",
    "    \n",
    "    sub_tensor = -1*weights[1]*torch.ones(N_CLASS, N_CLASS) + weights[0]*torch.eye(N_CLASS)\n",
    "    \n",
    "    out_saliency = F.relu(torch.einsum(\"ij,jklm->iklm\",\n",
    "                                       sub_tensor, \n",
    "                                       input_saliency))\n",
    "    return out_saliency.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5af449e9-c21f-48d4-a786-8f181e347e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_background(input_saliency, cls_label, thres=0.3):\n",
    "    '''\n",
    "    tensor -> ndarray\n",
    "    input_saliency.shape = Num_class, H, W\n",
    "    cls_label = sal_obj['cls_label']\n",
    "    predicted_saliency.shape = H, W\n",
    "    '''\n",
    "    \n",
    "    saliency = subtraction(input_saliency)\n",
    "    saliency_ndarray = normalization(saliency).cpu().numpy()\n",
    "    \n",
    "    saliency_bg = np.pad(saliency_ndarray, \n",
    "                  ((1, 0), (0, 0), (0, 0)), \n",
    "                  mode='constant', \n",
    "                  constant_values=thres)\n",
    "    \n",
    "    keys = np.pad(cls_label + 1, \n",
    "                  (1, 0), \n",
    "                  mode='constant')\n",
    "    \n",
    "    cls_labels = np.argmax(saliency_bg, axis=0)\n",
    "        \n",
    "    predicted_saliency = keys[cls_labels]\n",
    "    \n",
    "    return predicted_saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6930d3db-3d98-4d8b-bb46-ad7b47265a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_pixel_prediction(input_saliency, img_name, thres=0.1, fmt='absolute'):\n",
    "    '''\n",
    "    tensor -> ndarray\n",
    "    input_saliency.shape = Num_class, H, W\n",
    "    predicted_class.shape = H, W\n",
    "    '''\n",
    "    saliency_ndarray = input_saliency.cpu().numpy()\n",
    "    \n",
    "    N_CLASS, H, W = input_saliency.shape\n",
    "    \n",
    "    sp_onehot_mask = np.load(f\"/data/voc12/superpixel/masks/{img_name}.npy\")\n",
    "    \n",
    "    predicted_mask = [np.zeros_like(saliency_ndarray[0])+1e-8]\n",
    "    \n",
    "    for idx in range(N_CLASS):\n",
    "        \n",
    "        saliency = saliency_ndarray[idx]\n",
    "        scattered_saliency = sp_onehot_mask * saliency[None, :, :]\n",
    "        sp_score = scattered_saliency.sum((1, 2))/sp_onehot_mask.sum((1, 2))\n",
    "        \n",
    "        if fmt=='absolute':\n",
    "            selected_mask = sp_score > thres\n",
    "            \n",
    "        else:\n",
    "            selected_mask = sp_score > (thres * sp_score.mean())\n",
    "            \n",
    "        aggregated_saliency = scattered_saliency[selected_mask].sum(0)\n",
    "        \n",
    "        predicted_mask.append(aggregated_saliency)\n",
    "        \n",
    "    predicted_class = np.stack(predicted_mask).argmax(0)\n",
    "    \n",
    "    return predicted_class\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "028fc8b6-1324-41c5-a24a-a263eba0edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(saliency, class_label, thres=0.25, smoothing=False, superpixel=False):\n",
    "    \n",
    "        \n",
    "    keys = np.pad(class_label + 1, \n",
    "                  (1, 0), \n",
    "                  mode='constant')\n",
    "\n",
    "\n",
    "    if smoothing:\n",
    "\n",
    "        saliency_tensor = smooth(saliency)\n",
    "\n",
    "        saliency_tensor = normalization(saliency)\n",
    "\n",
    "    if superpixel:\n",
    "\n",
    "        saliency_tensor = subtraction(saliency)\n",
    "\n",
    "        saliency_tensor = normalization(saliency)\n",
    "\n",
    "        cls_labels = super_pixel_prediction(saliency, \n",
    "                                            img_name, \n",
    "                                            thres=thres, \n",
    "                                            fmt='absolute')\n",
    "        cls_labels = keys[cls_labels]\n",
    "\n",
    "        preds.append(cls_labels.copy())\n",
    "\n",
    "    else:\n",
    "        cls_labels = resolve_background(saliency, \n",
    "                                        class_label, \n",
    "                                        thres=thres)\n",
    "        return cls_labels.copy()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d3e6ac3-574f-45ec-b7c5-2e37b3ec6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_first_n_saliencies(options, n=1, thres=0.25):\n",
    "    \n",
    "    for iter, pack in enumerate(data_loader):\n",
    "        \n",
    "        img_name = pack['name'][0]\n",
    "        \n",
    "        input_img = pack['img'][0][0:1, ...]\n",
    "        \n",
    "        sal_obj = np.load(os.path.join(SALIENCY_DIR, img_name + '.npy'), allow_pickle=True).item()\n",
    "        \n",
    "        saliency = sal_obj['saliency']\n",
    "    \n",
    "        cls_label = sal_obj['cls_label']\n",
    "        \n",
    "        keys = np.pad(cls_label + 1, \n",
    "                      (1, 0), \n",
    "                      mode='constant')\n",
    "        \n",
    "        saliency_tensor = torch.tensor(saliency)\n",
    "        \n",
    "        preds = []\n",
    "        \n",
    "        if options['smoothing']:\n",
    "            \n",
    "            saliency_tensor = smooth(saliency_tensor)\n",
    "            \n",
    "            saliency_tensor = normalization(saliency_tensor)\n",
    "            \n",
    "        if options['superpixel']:\n",
    "            \n",
    "            saliency_tensor = subtraction(saliency_tensor)\n",
    "            \n",
    "            saliency_tensor = normalization(saliency_tensor)\n",
    "            \n",
    "            cls_labels = super_pixel_prediction(saliency_tensor, \n",
    "                                                img_name, \n",
    "                                                thres=thres, \n",
    "                                                fmt='absolute')\n",
    "            cls_labels = keys[cls_labels]\n",
    "            \n",
    "            preds.append(cls_labels.copy())\n",
    "            \n",
    "        else:\n",
    "            cls_labels = resolve_background(saliency_tensor, \n",
    "                                            cls_label, \n",
    "                                            thres=thres)\n",
    "            preds.append(cls_labels.copy())\n",
    "        \n",
    "        \n",
    "        predicted_saliency = preds[0]\n",
    "        gt_label = gt_labels[iter]\n",
    "        ndr_gt_label = ndr_gt_labels[iter]\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "        ax[0].imshow(myutils.unnormalize_image(input_img))\n",
    "        ax[0].axis('off')\n",
    "        ax[0].set_title('iou:{:.2f}\\nNDR-iou:{:.2f}'.format(compute_miou([predicted_saliency], [gt_label]), compute_miou([predicted_saliency], [ndr_gt_label])))\n",
    "        sns.heatmap(gt_label, vmax='20', cmap='tab20', ax=ax[1])\n",
    "        ax[1].axis('off')\n",
    "        ax[1].set_title('Ground Truth')\n",
    "        sns.heatmap(predicted_saliency, vmax='20', cmap='tab20', ax=ax[2])\n",
    "        ax[2].axis('off')\n",
    "        ax[2].set_title('Saliency prediction')\n",
    "        # plt.savefig('vis_'+str(count)+'.png')\n",
    "        plt.show()\n",
    "        \n",
    "        if iter+1 == n:\n",
    "            break\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce9889-8a4a-4351-b58b-9257a083e1ef",
   "metadata": {},
   "source": [
    "## miou functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d70e7d7-99f2-4fdd-b1f1-1a886b877c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainercv.evaluations import calc_semantic_segmentation_confusion\n",
    "\n",
    "def compute_miou(preds, gts):\n",
    "    \n",
    "    confusion = calc_semantic_segmentation_confusion(preds, gts)\n",
    "\n",
    "    gtj = confusion.sum(axis=1)\n",
    "    resj = confusion.sum(axis=0)\n",
    "    gtjresj = np.diag(confusion)\n",
    "    denominator = gtj + resj - gtjresj\n",
    "    iou = gtjresj / denominator\n",
    "\n",
    "    return np.nanmean(iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d37f7-1c9a-4b0f-be2b-640096b71b4a",
   "metadata": {},
   "source": [
    "## Generating Saliencies and Calculating mIOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "942591ce-5923-4f1c-acd3-65afc014ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_crops = [100,150,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b119dbe5-91b1-4496-9772-0628c7637704",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_set = [ 0.05, 0.1, 0.15, 0.2, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af42d0e2-7b77-4d04-9342-ac4ec716ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12465567-c378-4fea-beb4-e2a498b9b628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [19:42<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "100 Crops, Scale values: 0.05 - 0.10\n",
      "\n",
      "mIoU for 0.1: 0.3897053468272321\n",
      "NDR-mIoU for 0.1: 0.31365367140343414\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [19:44<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "100 Crops, Scale values: 0.05 - 0.15\n",
      "\n",
      "mIoU for 0.1: 0.39677079946092536\n",
      "NDR-mIoU for 0.1: 0.3340198776072462\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [17:23<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "100 Crops, Scale values: 0.05 - 0.20\n",
      "\n",
      "mIoU for 0.1: 0.3978762051085771\n",
      "NDR-mIoU for 0.1: 0.3442692825226056\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [14:55<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "100 Crops, Scale values: 0.05 - 0.30\n",
      "\n",
      "mIoU for 0.1: 0.39413382804285646\n",
      "NDR-mIoU for 0.1: 0.35409149241346227\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [14:53<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "100 Crops, Scale values: 0.10 - 0.15\n",
      "\n",
      "mIoU for 0.1: 0.3920197077280199\n",
      "NDR-mIoU for 0.1: 0.3349863901432553\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [14:54<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "100 Crops, Scale values: 0.10 - 0.20\n",
      "\n",
      "mIoU for 0.1: 0.3899059366394519\n",
      "NDR-mIoU for 0.1: 0.33999333745020416\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [14:44<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "100 Crops, Scale values: 0.10 - 0.30\n",
      "\n",
      "mIoU for 0.1: 0.38985591466539704\n",
      "NDR-mIoU for 0.1: 0.3553906066179039\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [14:57<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "100 Crops, Scale values: 0.15 - 0.20\n",
      "\n",
      "mIoU for 0.1: 0.3872541191861287\n",
      "NDR-mIoU for 0.1: 0.34318138273266224\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [14:37<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "100 Crops, Scale values: 0.15 - 0.30\n",
      "\n",
      "mIoU for 0.1: 0.38541864223907174\n",
      "NDR-mIoU for 0.1: 0.35411624955421095\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [14:32<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "100 Crops, Scale values: 0.20 - 0.30\n",
      "\n",
      "mIoU for 0.1: 0.38353860250418054\n",
      "NDR-mIoU for 0.1: 0.3564137545378112\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [21:04<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "150 Crops, Scale values: 0.05 - 0.10\n",
      "\n",
      "mIoU for 0.1: 0.4149798802744321\n",
      "NDR-mIoU for 0.1: 0.34331659211974913\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [21:07<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "150 Crops, Scale values: 0.05 - 0.15\n",
      "\n",
      "mIoU for 0.1: 0.4190450678442429\n",
      "NDR-mIoU for 0.1: 0.3609083430619779\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [21:17<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "150 Crops, Scale values: 0.05 - 0.20\n",
      "\n",
      "mIoU for 0.1: 0.41894381073588816\n",
      "NDR-mIoU for 0.1: 0.36989269257477303\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [20:54<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "150 Crops, Scale values: 0.05 - 0.30\n",
      "\n",
      "mIoU for 0.1: 0.4147343886139526\n",
      "NDR-mIoU for 0.1: 0.3827682529115089\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [20:07<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "150 Crops, Scale values: 0.10 - 0.15\n",
      "\n",
      "mIoU for 0.1: 0.4097510733418502\n",
      "NDR-mIoU for 0.1: 0.35554237923495674\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [20:07<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "150 Crops, Scale values: 0.10 - 0.20\n",
      "\n",
      "mIoU for 0.1: 0.4075458379323204\n",
      "NDR-mIoU for 0.1: 0.36018249665732993\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [20:14<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "150 Crops, Scale values: 0.10 - 0.30\n",
      "\n",
      "mIoU for 0.1: 0.4053612953474724\n",
      "NDR-mIoU for 0.1: 0.37360108709174594\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [20:10<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "150 Crops, Scale values: 0.15 - 0.20\n",
      "\n",
      "mIoU for 0.1: 0.400237419298537\n",
      "NDR-mIoU for 0.1: 0.35966822446430424\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [20:17<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "150 Crops, Scale values: 0.15 - 0.30\n",
      "\n",
      "mIoU for 0.1: 0.39779527820322597\n",
      "NDR-mIoU for 0.1: 0.3688219263867585\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [20:12<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "150 Crops, Scale values: 0.20 - 0.30\n",
      "\n",
      "mIoU for 0.1: 0.39331357144128193\n",
      "NDR-mIoU for 0.1: 0.36978445951410666\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [25:57<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "200 Crops, Scale values: 0.05 - 0.10\n",
      "\n",
      "mIoU for 0.1: 0.4135131414665482\n",
      "NDR-mIoU for 0.1: 0.3391015702347862\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [26:12<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "200 Crops, Scale values: 0.05 - 0.15\n",
      "\n",
      "mIoU for 0.1: 0.41687850375547886\n",
      "NDR-mIoU for 0.1: 0.35577681377367576\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [26:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "200 Crops, Scale values: 0.05 - 0.20\n",
      "\n",
      "mIoU for 0.1: 0.4164293152390105\n",
      "NDR-mIoU for 0.1: 0.36568868711150404\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [26:26<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "200 Crops, Scale values: 0.05 - 0.30\n",
      "\n",
      "mIoU for 0.1: 0.41217884335488497\n",
      "NDR-mIoU for 0.1: 0.3767364313691243\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [26:13<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "200 Crops, Scale values: 0.10 - 0.15\n",
      "\n",
      "mIoU for 0.1: 0.4070248574246493\n",
      "NDR-mIoU for 0.1: 0.35068715693680985\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1377/1464 [24:33<01:33,  1.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-16cae763826d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                                   \u001b[0mtarget_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                                   \u001b[0minput_fmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'crop_accumulate'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_crops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_crop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                                                   num_masks=NUM_MASKS)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                         \u001b[0;31m#Saving salienices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3fa0a89bfbad>\u001b[0m in \u001b[0;36mget_saliencies\u001b[0;34m(img_tensor, target_class, transform, input_fmt, num_crops, num_masks)\u001b[0m\n\u001b[1;32m     75\u001b[0m                                         \u001b[0mtarget_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                                         \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                                         \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                                     )[0]\n\u001b[1;32m     79\u001b[0m                 \u001b[0msaliency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaliency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    234\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n_crop in pos_crops:\n",
    "\n",
    "    for i in range(len(scale_set)-1):\n",
    "        \n",
    "        scale1 = scale_set[i]\n",
    "        \n",
    "        for c in range(1, len(scale_set)- i):\n",
    "            \n",
    "            scale2 = scale_set[i+c]\n",
    "            \n",
    "\n",
    "            t = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(\n",
    "                (224,224),\n",
    "                scale=(scale1, scale2), \n",
    "                ratio=(1, 1)\n",
    "            )\n",
    "            ])\n",
    "            \n",
    "            predictions = []\n",
    "            print(\"Generating Saliencies\")\n",
    "            \n",
    "            for iter, pack in enumerate(tqdm(data_loader)):\n",
    "                    img_name = pack['name'][0]\n",
    "\n",
    "\n",
    "                    label = pack['label'][0]\n",
    "\n",
    "                    target_class = torch.nonzero(label)[:, 0]\n",
    "                    number_classes = int(label.sum().item())\n",
    "\n",
    "                    input_img = pack['img'][0][0:1, ...]\n",
    "\n",
    "                    gt_label = gt_labels[iter]\n",
    "\n",
    "                    with torch.autocast(device_type = 'cuda', dtype = torch.float16):\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        accumulated_saliency = get_saliencies(img_tensor=input_img,\n",
    "                                                  target_class=target_class, transform = t,\n",
    "                                                  input_fmt='crop_accumulate',num_crops=n_crop,\n",
    "                                                  num_masks=NUM_MASKS)\n",
    "                        \n",
    "                        #Saving salienices\n",
    "                        np.save(os.path.join(SALIENCY_DIR, img_name+\"_\"+str(n_crop)+\"_\"+str(scale1)+\"_\"+str(scale2)+'.npy'),\n",
    "                                {'saliency': accumulated_saliency.cpu().numpy(), 'cls_label': target_class})\n",
    "                    \n",
    "    \n",
    "                        preds = get_predictions(accumulated_saliency.cpu(),target_class, thres=thres, smoothing=False, superpixel=False)\n",
    "                        predictions.append(preds)\n",
    "                        \n",
    "                        \n",
    "            print(\"Calculating miou\")\n",
    "            print(f'\\n{\"\".join([\"#\" for i in range(60)])}')\n",
    "            print('SMOOTHING: False, SUPERPIXEL: False')\n",
    "            print(f'{\"\".join([\"#\" for i in range(60)])}\\n')\n",
    "            \n",
    "        \n",
    "            miou = compute_miou(predictions, gt_labels)\n",
    "            ndr_miou = compute_miou(predictions, ndr_gt_labels)\n",
    "            print('{:.0f} Crops, Scale values: {:.2f} - {:.2f}'.format(n_crop, scale1, scale2))\n",
    "            print(f'\\nmIoU for {thres}: {miou}\\nNDR-mIoU for {thres}: {ndr_miou}\\n\\n')\n",
    "                        \n",
    "                    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965b07e-cc71-4a1a-b902-892f5480162c",
   "metadata": {},
   "source": [
    "### Continuing where it stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ead76ef-3837-41c3-99e2-4a162cc934ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_crops = [200]\n",
    "scale_set = [0.1,0.15,0.2,0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e50fad7-274b-44ec-b2c3-92d032b1e8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [26:26<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "200 Crops, Scale values: 0.10 - 0.15\n",
      "\n",
      "mIoU for 0.1: 0.40700938847169416\n",
      "NDR-mIoU for 0.1: 0.35022285122579\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [26:28<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "200 Crops, Scale values: 0.10 - 0.20\n",
      "\n",
      "mIoU for 0.1: 0.4055767052030322\n",
      "NDR-mIoU for 0.1: 0.35668100559230226\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [26:44<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "200 Crops, Scale values: 0.10 - 0.30\n",
      "\n",
      "mIoU for 0.1: 0.4032739996417058\n",
      "NDR-mIoU for 0.1: 0.36862630126429785\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [26:33<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "200 Crops, Scale values: 0.15 - 0.20\n",
      "\n",
      "mIoU for 0.1: 0.39757061246857406\n",
      "NDR-mIoU for 0.1: 0.3539242905483527\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [26:35<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "200 Crops, Scale values: 0.15 - 0.30\n",
      "\n",
      "mIoU for 0.1: 0.3962864069065184\n",
      "NDR-mIoU for 0.1: 0.3658672157969954\n",
      "\n",
      "\n",
      "Generating Saliencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1464/1464 [26:47<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating miou\n",
      "\n",
      "############################################################\n",
      "SMOOTHING: False, SUPERPIXEL: False\n",
      "############################################################\n",
      "\n",
      "200 Crops, Scale values: 0.20 - 0.30\n",
      "\n",
      "mIoU for 0.1: 0.3916615274964529\n",
      "NDR-mIoU for 0.1: 0.3660283623432209\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n_crop in pos_crops:\n",
    "\n",
    "    for i in range(len(scale_set)-1):\n",
    "        \n",
    "        scale1 = scale_set[i]\n",
    "        \n",
    "        for c in range(1, len(scale_set)- i):\n",
    "            \n",
    "            scale2 = scale_set[i+c]\n",
    "            \n",
    "\n",
    "            t = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(\n",
    "                (224,224),\n",
    "                scale=(scale1, scale2), \n",
    "                ratio=(1, 1)\n",
    "            )\n",
    "            ])\n",
    "            \n",
    "            predictions = []\n",
    "            print(\"Generating Saliencies\")\n",
    "            \n",
    "            for iter, pack in enumerate(tqdm(data_loader)):\n",
    "                    img_name = pack['name'][0]\n",
    "\n",
    "\n",
    "                    label = pack['label'][0]\n",
    "\n",
    "                    target_class = torch.nonzero(label)[:, 0]\n",
    "                    number_classes = int(label.sum().item())\n",
    "\n",
    "                    input_img = pack['img'][0][0:1, ...]\n",
    "\n",
    "                    gt_label = gt_labels[iter]\n",
    "\n",
    "                    with torch.autocast(device_type = 'cuda', dtype = torch.float16):\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        accumulated_saliency = get_saliencies(img_tensor=input_img,\n",
    "                                                  target_class=target_class, transform = t,\n",
    "                                                  input_fmt='crop_accumulate',num_crops=n_crop,\n",
    "                                                  num_masks=NUM_MASKS)\n",
    "                        \n",
    "                        #Saving salienices\n",
    "                        np.save(os.path.join(SALIENCY_DIR, img_name+\"_\"+str(n_crop)+\"_\"+str(scale1)+\"_\"+str(scale2)+'.npy'),\n",
    "                                {'saliency': accumulated_saliency.cpu().numpy(), 'cls_label': target_class})\n",
    "                    \n",
    "    \n",
    "                        preds = get_predictions(accumulated_saliency.cpu(),target_class, thres=thres, smoothing=False, superpixel=False)\n",
    "                        predictions.append(preds)\n",
    "                        \n",
    "                        \n",
    "            print(\"Calculating miou\")\n",
    "            print(f'\\n{\"\".join([\"#\" for i in range(60)])}')\n",
    "            print('SMOOTHING: False, SUPERPIXEL: False')\n",
    "            print(f'{\"\".join([\"#\" for i in range(60)])}\\n')\n",
    "            \n",
    "        \n",
    "            miou = compute_miou(predictions, gt_labels)\n",
    "            ndr_miou = compute_miou(predictions, ndr_gt_labels)\n",
    "            print('{:.0f} Crops, Scale values: {:.2f} - {:.2f}'.format(n_crop, scale1, scale2))\n",
    "            print(f'\\nmIoU for {thres}: {miou}\\nNDR-mIoU for {thres}: {ndr_miou}\\n\\n')\n",
    "                        \n",
    "                    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7fb68-dc63-46a2-91bb-e9dc856ff0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
